{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/akshay/anaconda3/lib/python3.6/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.9.0.so'\n",
      "WARNING:root:Falling back to using int100 tensors due to lack of int64 support. Performance may be improved by installing a version of TensorFlow supporting this (1.13+ or custom build).\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Create a couple of workers\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  \n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff3af4101b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FederatedDataLoader \n",
    "# we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]) \n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/F_MNIST_data/', train=True, download=True, transform=transform).federate((bob, alice))\n",
    "mnist_testset = datasets.MNIST('~/.pytorch/F_MNIST_data/', train=False, download=True, transform=transform)\n",
    "\n",
    "federated_train_loader = sy.FederatedDataLoader(mnist_trainset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=args.test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [Training: 0%]\tLoss: 2.307303\n",
      "Epoch: 1 [Training: 3%]\tLoss: 2.152380\n",
      "Epoch: 1 [Training: 6%]\tLoss: 1.851272\n",
      "Epoch: 1 [Training: 10%]\tLoss: 1.258177\n",
      "Epoch: 1 [Training: 13%]\tLoss: 0.785922\n",
      "Epoch: 1 [Training: 16%]\tLoss: 0.752820\n",
      "Epoch: 1 [Training: 19%]\tLoss: 0.389547\n",
      "Epoch: 1 [Training: 22%]\tLoss: 0.668079\n",
      "Epoch: 1 [Training: 26%]\tLoss: 0.556559\n",
      "Epoch: 1 [Training: 29%]\tLoss: 0.298134\n",
      "Epoch: 1 [Training: 32%]\tLoss: 0.709528\n",
      "Epoch: 1 [Training: 35%]\tLoss: 0.243506\n",
      "Epoch: 1 [Training: 38%]\tLoss: 0.295973\n",
      "Epoch: 1 [Training: 42%]\tLoss: 0.437727\n",
      "Epoch: 1 [Training: 45%]\tLoss: 0.444640\n",
      "Epoch: 1 [Training: 48%]\tLoss: 0.462919\n",
      "Epoch: 1 [Training: 51%]\tLoss: 0.377572\n",
      "Epoch: 1 [Training: 54%]\tLoss: 0.231817\n",
      "Epoch: 1 [Training: 58%]\tLoss: 0.164112\n",
      "Epoch: 1 [Training: 61%]\tLoss: 0.186264\n",
      "Epoch: 1 [Training: 64%]\tLoss: 0.267224\n",
      "Epoch: 1 [Training: 67%]\tLoss: 0.161641\n",
      "Epoch: 1 [Training: 70%]\tLoss: 0.223377\n",
      "Epoch: 1 [Training: 74%]\tLoss: 0.181561\n",
      "Epoch: 1 [Training: 77%]\tLoss: 0.177792\n",
      "Epoch: 1 [Training: 80%]\tLoss: 0.136199\n",
      "Epoch: 1 [Training: 83%]\tLoss: 0.146292\n",
      "Epoch: 1 [Training: 86%]\tLoss: 0.195848\n",
      "Epoch: 1 [Training: 90%]\tLoss: 0.309594\n",
      "Epoch: 1 [Training: 93%]\tLoss: 0.128701\n",
      "Epoch: 1 [Training: 96%]\tLoss: 0.145889\n",
      "Epoch: 1 [Training: 99%]\tLoss: 0.104276\n",
      "\n",
      "Test set: Average loss: 0.1697, Accuracy: 9480/10000 (95%)\n",
      "\n",
      "Epoch: 2 [Training: 0%]\tLoss: 0.108670\n",
      "Epoch: 2 [Training: 3%]\tLoss: 0.266702\n",
      "Epoch: 2 [Training: 6%]\tLoss: 0.050081\n",
      "Epoch: 2 [Training: 10%]\tLoss: 0.287172\n",
      "Epoch: 2 [Training: 13%]\tLoss: 0.163822\n",
      "Epoch: 2 [Training: 16%]\tLoss: 0.207235\n",
      "Epoch: 2 [Training: 19%]\tLoss: 0.149060\n",
      "Epoch: 2 [Training: 22%]\tLoss: 0.146855\n",
      "Epoch: 2 [Training: 26%]\tLoss: 0.127540\n",
      "Epoch: 2 [Training: 29%]\tLoss: 0.165249\n",
      "Epoch: 2 [Training: 32%]\tLoss: 0.272708\n",
      "Epoch: 2 [Training: 35%]\tLoss: 0.069189\n",
      "Epoch: 2 [Training: 38%]\tLoss: 0.108421\n",
      "Epoch: 2 [Training: 42%]\tLoss: 0.222402\n",
      "Epoch: 2 [Training: 45%]\tLoss: 0.169010\n",
      "Epoch: 2 [Training: 48%]\tLoss: 0.165265\n",
      "Epoch: 2 [Training: 51%]\tLoss: 0.174334\n",
      "Epoch: 2 [Training: 54%]\tLoss: 0.084253\n",
      "Epoch: 2 [Training: 58%]\tLoss: 0.094128\n",
      "Epoch: 2 [Training: 61%]\tLoss: 0.114865\n",
      "Epoch: 2 [Training: 64%]\tLoss: 0.050225\n",
      "Epoch: 2 [Training: 67%]\tLoss: 0.098143\n",
      "Epoch: 2 [Training: 70%]\tLoss: 0.214611\n",
      "Epoch: 2 [Training: 74%]\tLoss: 0.095112\n",
      "Epoch: 2 [Training: 77%]\tLoss: 0.136437\n",
      "Epoch: 2 [Training: 80%]\tLoss: 0.161120\n",
      "Epoch: 2 [Training: 83%]\tLoss: 0.035297\n",
      "Epoch: 2 [Training: 86%]\tLoss: 0.084341\n",
      "Epoch: 2 [Training: 90%]\tLoss: 0.041573\n",
      "Epoch: 2 [Training: 93%]\tLoss: 0.200118\n",
      "Epoch: 2 [Training: 96%]\tLoss: 0.042420\n",
      "Epoch: 2 [Training: 99%]\tLoss: 0.111339\n",
      "\n",
      "Test set: Average loss: 0.0884, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "Epoch: 3 [Training: 0%]\tLoss: 0.048069\n",
      "Epoch: 3 [Training: 3%]\tLoss: 0.148299\n",
      "Epoch: 3 [Training: 6%]\tLoss: 0.050006\n",
      "Epoch: 3 [Training: 10%]\tLoss: 0.048210\n",
      "Epoch: 3 [Training: 13%]\tLoss: 0.100720\n",
      "Epoch: 3 [Training: 16%]\tLoss: 0.077323\n",
      "Epoch: 3 [Training: 19%]\tLoss: 0.090851\n",
      "Epoch: 3 [Training: 22%]\tLoss: 0.206647\n",
      "Epoch: 3 [Training: 26%]\tLoss: 0.102429\n",
      "Epoch: 3 [Training: 29%]\tLoss: 0.013528\n",
      "Epoch: 3 [Training: 32%]\tLoss: 0.125367\n",
      "Epoch: 3 [Training: 35%]\tLoss: 0.167114\n",
      "Epoch: 3 [Training: 38%]\tLoss: 0.103664\n",
      "Epoch: 3 [Training: 42%]\tLoss: 0.045010\n",
      "Epoch: 3 [Training: 45%]\tLoss: 0.079995\n",
      "Epoch: 3 [Training: 48%]\tLoss: 0.058574\n",
      "Epoch: 3 [Training: 51%]\tLoss: 0.160364\n",
      "Epoch: 3 [Training: 54%]\tLoss: 0.068849\n",
      "Epoch: 3 [Training: 58%]\tLoss: 0.039022\n",
      "Epoch: 3 [Training: 61%]\tLoss: 0.073899\n",
      "Epoch: 3 [Training: 64%]\tLoss: 0.060212\n",
      "Epoch: 3 [Training: 67%]\tLoss: 0.123528\n",
      "Epoch: 3 [Training: 70%]\tLoss: 0.033860\n",
      "Epoch: 3 [Training: 74%]\tLoss: 0.075081\n",
      "Epoch: 3 [Training: 77%]\tLoss: 0.088478\n",
      "Epoch: 3 [Training: 80%]\tLoss: 0.069325\n",
      "Epoch: 3 [Training: 83%]\tLoss: 0.194571\n",
      "Epoch: 3 [Training: 86%]\tLoss: 0.084805\n",
      "Epoch: 3 [Training: 90%]\tLoss: 0.041522\n",
      "Epoch: 3 [Training: 93%]\tLoss: 0.031207\n",
      "Epoch: 3 [Training: 96%]\tLoss: 0.023709\n",
      "Epoch: 3 [Training: 99%]\tLoss: 0.008709\n",
      "\n",
      "Test set: Average loss: 0.0657, Accuracy: 9806/10000 (98%)\n",
      "\n",
      "Epoch: 4 [Training: 0%]\tLoss: 0.143658\n",
      "Epoch: 4 [Training: 3%]\tLoss: 0.013815\n",
      "Epoch: 4 [Training: 6%]\tLoss: 0.059316\n",
      "Epoch: 4 [Training: 10%]\tLoss: 0.041401\n",
      "Epoch: 4 [Training: 13%]\tLoss: 0.110626\n",
      "Epoch: 4 [Training: 16%]\tLoss: 0.064643\n",
      "Epoch: 4 [Training: 19%]\tLoss: 0.135767\n",
      "Epoch: 4 [Training: 22%]\tLoss: 0.070887\n",
      "Epoch: 4 [Training: 26%]\tLoss: 0.054087\n",
      "Epoch: 4 [Training: 29%]\tLoss: 0.031058\n",
      "Epoch: 4 [Training: 32%]\tLoss: 0.049514\n",
      "Epoch: 4 [Training: 35%]\tLoss: 0.059045\n",
      "Epoch: 4 [Training: 38%]\tLoss: 0.022684\n",
      "Epoch: 4 [Training: 42%]\tLoss: 0.028310\n",
      "Epoch: 4 [Training: 45%]\tLoss: 0.064079\n",
      "Epoch: 4 [Training: 48%]\tLoss: 0.040974\n",
      "Epoch: 4 [Training: 51%]\tLoss: 0.069013\n",
      "Epoch: 4 [Training: 54%]\tLoss: 0.101540\n",
      "Epoch: 4 [Training: 58%]\tLoss: 0.030434\n",
      "Epoch: 4 [Training: 61%]\tLoss: 0.015278\n",
      "Epoch: 4 [Training: 64%]\tLoss: 0.018080\n",
      "Epoch: 4 [Training: 67%]\tLoss: 0.021769\n",
      "Epoch: 4 [Training: 70%]\tLoss: 0.048786\n",
      "Epoch: 4 [Training: 74%]\tLoss: 0.018423\n",
      "Epoch: 4 [Training: 77%]\tLoss: 0.043589\n",
      "Epoch: 4 [Training: 80%]\tLoss: 0.041111\n",
      "Epoch: 4 [Training: 83%]\tLoss: 0.035982\n",
      "Epoch: 4 [Training: 86%]\tLoss: 0.086079\n",
      "Epoch: 4 [Training: 90%]\tLoss: 0.071461\n",
      "Epoch: 4 [Training: 93%]\tLoss: 0.025262\n",
      "Epoch: 4 [Training: 96%]\tLoss: 0.138801\n",
      "Epoch: 4 [Training: 99%]\tLoss: 0.067465\n",
      "\n",
      "Test set: Average loss: 0.0540, Accuracy: 9829/10000 (98%)\n",
      "\n",
      "Epoch: 5 [Training: 0%]\tLoss: 0.080805\n",
      "Epoch: 5 [Training: 3%]\tLoss: 0.040142\n",
      "Epoch: 5 [Training: 6%]\tLoss: 0.068471\n",
      "Epoch: 5 [Training: 10%]\tLoss: 0.162296\n",
      "Epoch: 5 [Training: 13%]\tLoss: 0.028187\n",
      "Epoch: 5 [Training: 16%]\tLoss: 0.019124\n",
      "Epoch: 5 [Training: 19%]\tLoss: 0.044831\n",
      "Epoch: 5 [Training: 22%]\tLoss: 0.014240\n",
      "Epoch: 5 [Training: 26%]\tLoss: 0.043607\n",
      "Epoch: 5 [Training: 29%]\tLoss: 0.111328\n",
      "Epoch: 5 [Training: 32%]\tLoss: 0.047142\n",
      "Epoch: 5 [Training: 35%]\tLoss: 0.016166\n",
      "Epoch: 5 [Training: 38%]\tLoss: 0.112859\n",
      "Epoch: 5 [Training: 42%]\tLoss: 0.128347\n",
      "Epoch: 5 [Training: 45%]\tLoss: 0.081234\n",
      "Epoch: 5 [Training: 48%]\tLoss: 0.049344\n",
      "Epoch: 5 [Training: 51%]\tLoss: 0.032253\n",
      "Epoch: 5 [Training: 54%]\tLoss: 0.044587\n",
      "Epoch: 5 [Training: 58%]\tLoss: 0.072438\n",
      "Epoch: 5 [Training: 61%]\tLoss: 0.110039\n",
      "Epoch: 5 [Training: 64%]\tLoss: 0.051544\n",
      "Epoch: 5 [Training: 67%]\tLoss: 0.021044\n",
      "Epoch: 5 [Training: 70%]\tLoss: 0.007800\n",
      "Epoch: 5 [Training: 74%]\tLoss: 0.047660\n",
      "Epoch: 5 [Training: 77%]\tLoss: 0.032039\n",
      "Epoch: 5 [Training: 80%]\tLoss: 0.032558\n",
      "Epoch: 5 [Training: 83%]\tLoss: 0.037076\n",
      "Epoch: 5 [Training: 86%]\tLoss: 0.014250\n",
      "Epoch: 5 [Training: 90%]\tLoss: 0.082118\n",
      "Epoch: 5 [Training: 93%]\tLoss: 0.060568\n",
      "Epoch: 5 [Training: 96%]\tLoss: 0.164142\n",
      "Epoch: 5 [Training: 99%]\tLoss: 0.064679\n",
      "\n",
      "Test set: Average loss: 0.0512, Accuracy: 9834/10000 (98%)\n",
      "\n",
      "Epoch: 6 [Training: 0%]\tLoss: 0.018779\n",
      "Epoch: 6 [Training: 3%]\tLoss: 0.004002\n",
      "Epoch: 6 [Training: 6%]\tLoss: 0.023783\n",
      "Epoch: 6 [Training: 10%]\tLoss: 0.104266\n",
      "Epoch: 6 [Training: 13%]\tLoss: 0.025700\n",
      "Epoch: 6 [Training: 16%]\tLoss: 0.144293\n",
      "Epoch: 6 [Training: 19%]\tLoss: 0.040913\n",
      "Epoch: 6 [Training: 22%]\tLoss: 0.039881\n",
      "Epoch: 6 [Training: 26%]\tLoss: 0.023718\n",
      "Epoch: 6 [Training: 29%]\tLoss: 0.022758\n",
      "Epoch: 6 [Training: 32%]\tLoss: 0.116088\n",
      "Epoch: 6 [Training: 35%]\tLoss: 0.019766\n",
      "Epoch: 6 [Training: 38%]\tLoss: 0.068523\n",
      "Epoch: 6 [Training: 42%]\tLoss: 0.065363\n",
      "Epoch: 6 [Training: 45%]\tLoss: 0.021900\n",
      "Epoch: 6 [Training: 48%]\tLoss: 0.010512\n",
      "Epoch: 6 [Training: 51%]\tLoss: 0.057751\n",
      "Epoch: 6 [Training: 54%]\tLoss: 0.035972\n",
      "Epoch: 6 [Training: 58%]\tLoss: 0.034201\n",
      "Epoch: 6 [Training: 61%]\tLoss: 0.007046\n",
      "Epoch: 6 [Training: 64%]\tLoss: 0.032658\n",
      "Epoch: 6 [Training: 67%]\tLoss: 0.072303\n",
      "Epoch: 6 [Training: 70%]\tLoss: 0.034935\n",
      "Epoch: 6 [Training: 74%]\tLoss: 0.019853\n",
      "Epoch: 6 [Training: 77%]\tLoss: 0.019100\n",
      "Epoch: 6 [Training: 80%]\tLoss: 0.088919\n",
      "Epoch: 6 [Training: 83%]\tLoss: 0.059503\n",
      "Epoch: 6 [Training: 86%]\tLoss: 0.050164\n",
      "Epoch: 6 [Training: 90%]\tLoss: 0.004906\n",
      "Epoch: 6 [Training: 93%]\tLoss: 0.034103\n",
      "Epoch: 6 [Training: 96%]\tLoss: 0.047512\n",
      "Epoch: 6 [Training: 99%]\tLoss: 0.019925\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9855/10000 (99%)\n",
      "\n",
      "Epoch: 7 [Training: 0%]\tLoss: 0.056761\n",
      "Epoch: 7 [Training: 3%]\tLoss: 0.032007\n",
      "Epoch: 7 [Training: 6%]\tLoss: 0.071122\n",
      "Epoch: 7 [Training: 10%]\tLoss: 0.017573\n",
      "Epoch: 7 [Training: 13%]\tLoss: 0.009697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 [Training: 16%]\tLoss: 0.039918\n",
      "Epoch: 7 [Training: 19%]\tLoss: 0.014348\n",
      "Epoch: 7 [Training: 22%]\tLoss: 0.074999\n",
      "Epoch: 7 [Training: 26%]\tLoss: 0.004217\n",
      "Epoch: 7 [Training: 29%]\tLoss: 0.026987\n",
      "Epoch: 7 [Training: 32%]\tLoss: 0.027636\n",
      "Epoch: 7 [Training: 35%]\tLoss: 0.015602\n",
      "Epoch: 7 [Training: 38%]\tLoss: 0.088973\n",
      "Epoch: 7 [Training: 42%]\tLoss: 0.025428\n",
      "Epoch: 7 [Training: 45%]\tLoss: 0.015774\n",
      "Epoch: 7 [Training: 48%]\tLoss: 0.088870\n",
      "Epoch: 7 [Training: 51%]\tLoss: 0.054397\n",
      "Epoch: 7 [Training: 54%]\tLoss: 0.055844\n",
      "Epoch: 7 [Training: 58%]\tLoss: 0.004940\n",
      "Epoch: 7 [Training: 61%]\tLoss: 0.016300\n",
      "Epoch: 7 [Training: 64%]\tLoss: 0.004413\n",
      "Epoch: 7 [Training: 67%]\tLoss: 0.080199\n",
      "Epoch: 7 [Training: 70%]\tLoss: 0.011111\n",
      "Epoch: 7 [Training: 74%]\tLoss: 0.062064\n",
      "Epoch: 7 [Training: 77%]\tLoss: 0.109649\n",
      "Epoch: 7 [Training: 80%]\tLoss: 0.097691\n",
      "Epoch: 7 [Training: 83%]\tLoss: 0.021317\n",
      "Epoch: 7 [Training: 86%]\tLoss: 0.023506\n",
      "Epoch: 7 [Training: 90%]\tLoss: 0.015856\n",
      "Epoch: 7 [Training: 93%]\tLoss: 0.066526\n",
      "Epoch: 7 [Training: 96%]\tLoss: 0.110015\n",
      "Epoch: 7 [Training: 99%]\tLoss: 0.053823\n",
      "\n",
      "Test set: Average loss: 0.0399, Accuracy: 9872/10000 (99%)\n",
      "\n",
      "Epoch: 8 [Training: 0%]\tLoss: 0.018393\n",
      "Epoch: 8 [Training: 3%]\tLoss: 0.099845\n",
      "Epoch: 8 [Training: 6%]\tLoss: 0.032879\n",
      "Epoch: 8 [Training: 10%]\tLoss: 0.018595\n",
      "Epoch: 8 [Training: 13%]\tLoss: 0.083990\n",
      "Epoch: 8 [Training: 16%]\tLoss: 0.041493\n",
      "Epoch: 8 [Training: 19%]\tLoss: 0.031377\n",
      "Epoch: 8 [Training: 22%]\tLoss: 0.035733\n",
      "Epoch: 8 [Training: 26%]\tLoss: 0.027596\n",
      "Epoch: 8 [Training: 29%]\tLoss: 0.018879\n",
      "Epoch: 8 [Training: 32%]\tLoss: 0.033990\n",
      "Epoch: 8 [Training: 35%]\tLoss: 0.009537\n",
      "Epoch: 8 [Training: 38%]\tLoss: 0.042119\n",
      "Epoch: 8 [Training: 42%]\tLoss: 0.053426\n",
      "Epoch: 8 [Training: 45%]\tLoss: 0.007157\n",
      "Epoch: 8 [Training: 48%]\tLoss: 0.022497\n",
      "Epoch: 8 [Training: 51%]\tLoss: 0.013407\n",
      "Epoch: 8 [Training: 54%]\tLoss: 0.050486\n",
      "Epoch: 8 [Training: 58%]\tLoss: 0.052039\n",
      "Epoch: 8 [Training: 61%]\tLoss: 0.014872\n",
      "Epoch: 8 [Training: 64%]\tLoss: 0.085401\n",
      "Epoch: 8 [Training: 67%]\tLoss: 0.064143\n",
      "Epoch: 8 [Training: 70%]\tLoss: 0.005435\n",
      "Epoch: 8 [Training: 74%]\tLoss: 0.180783\n",
      "Epoch: 8 [Training: 77%]\tLoss: 0.024287\n",
      "Epoch: 8 [Training: 80%]\tLoss: 0.006641\n",
      "Epoch: 8 [Training: 83%]\tLoss: 0.009455\n",
      "Epoch: 8 [Training: 86%]\tLoss: 0.046895\n",
      "Epoch: 8 [Training: 90%]\tLoss: 0.010581\n",
      "Epoch: 8 [Training: 93%]\tLoss: 0.009598\n",
      "Epoch: 8 [Training: 96%]\tLoss: 0.137080\n",
      "Epoch: 8 [Training: 99%]\tLoss: 0.025167\n",
      "\n",
      "Test set: Average loss: 0.0487, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Epoch: 9 [Training: 0%]\tLoss: 0.010458\n",
      "Epoch: 9 [Training: 3%]\tLoss: 0.043057\n",
      "Epoch: 9 [Training: 6%]\tLoss: 0.014211\n",
      "Epoch: 9 [Training: 10%]\tLoss: 0.005317\n",
      "Epoch: 9 [Training: 13%]\tLoss: 0.030647\n",
      "Epoch: 9 [Training: 16%]\tLoss: 0.017548\n",
      "Epoch: 9 [Training: 19%]\tLoss: 0.005079\n",
      "Epoch: 9 [Training: 22%]\tLoss: 0.015811\n",
      "Epoch: 9 [Training: 26%]\tLoss: 0.021837\n",
      "Epoch: 9 [Training: 29%]\tLoss: 0.026996\n",
      "Epoch: 9 [Training: 32%]\tLoss: 0.031175\n",
      "Epoch: 9 [Training: 35%]\tLoss: 0.132813\n",
      "Epoch: 9 [Training: 38%]\tLoss: 0.013059\n",
      "Epoch: 9 [Training: 42%]\tLoss: 0.003998\n",
      "Epoch: 9 [Training: 45%]\tLoss: 0.005980\n",
      "Epoch: 9 [Training: 48%]\tLoss: 0.064272\n",
      "Epoch: 9 [Training: 51%]\tLoss: 0.032403\n",
      "Epoch: 9 [Training: 54%]\tLoss: 0.011036\n",
      "Epoch: 9 [Training: 58%]\tLoss: 0.012062\n",
      "Epoch: 9 [Training: 61%]\tLoss: 0.011592\n",
      "Epoch: 9 [Training: 64%]\tLoss: 0.055528\n",
      "Epoch: 9 [Training: 67%]\tLoss: 0.063919\n",
      "Epoch: 9 [Training: 70%]\tLoss: 0.056884\n",
      "Epoch: 9 [Training: 74%]\tLoss: 0.084084\n",
      "Epoch: 9 [Training: 77%]\tLoss: 0.011614\n",
      "Epoch: 9 [Training: 80%]\tLoss: 0.013433\n",
      "Epoch: 9 [Training: 83%]\tLoss: 0.070815\n",
      "Epoch: 9 [Training: 86%]\tLoss: 0.039090\n",
      "Epoch: 9 [Training: 90%]\tLoss: 0.026005\n",
      "Epoch: 9 [Training: 93%]\tLoss: 0.078238\n",
      "Epoch: 9 [Training: 96%]\tLoss: 0.011645\n",
      "Epoch: 9 [Training: 99%]\tLoss: 0.044914\n",
      "\n",
      "Test set: Average loss: 0.0422, Accuracy: 9870/10000 (99%)\n",
      "\n",
      "Epoch: 10 [Training: 0%]\tLoss: 0.098335\n",
      "Epoch: 10 [Training: 3%]\tLoss: 0.008261\n",
      "Epoch: 10 [Training: 6%]\tLoss: 0.026815\n",
      "Epoch: 10 [Training: 10%]\tLoss: 0.031150\n",
      "Epoch: 10 [Training: 13%]\tLoss: 0.012635\n",
      "Epoch: 10 [Training: 16%]\tLoss: 0.017256\n",
      "Epoch: 10 [Training: 19%]\tLoss: 0.026883\n",
      "Epoch: 10 [Training: 22%]\tLoss: 0.107162\n",
      "Epoch: 10 [Training: 26%]\tLoss: 0.022964\n",
      "Epoch: 10 [Training: 29%]\tLoss: 0.009551\n",
      "Epoch: 10 [Training: 32%]\tLoss: 0.007119\n",
      "Epoch: 10 [Training: 35%]\tLoss: 0.003208\n",
      "Epoch: 10 [Training: 38%]\tLoss: 0.008164\n",
      "Epoch: 10 [Training: 42%]\tLoss: 0.029331\n",
      "Epoch: 10 [Training: 45%]\tLoss: 0.009460\n",
      "Epoch: 10 [Training: 48%]\tLoss: 0.024564\n",
      "Epoch: 10 [Training: 51%]\tLoss: 0.047982\n",
      "Epoch: 10 [Training: 54%]\tLoss: 0.040244\n",
      "Epoch: 10 [Training: 58%]\tLoss: 0.012489\n",
      "Epoch: 10 [Training: 61%]\tLoss: 0.022150\n",
      "Epoch: 10 [Training: 64%]\tLoss: 0.005532\n",
      "Epoch: 10 [Training: 67%]\tLoss: 0.016049\n",
      "Epoch: 10 [Training: 70%]\tLoss: 0.003687\n",
      "Epoch: 10 [Training: 74%]\tLoss: 0.013515\n",
      "Epoch: 10 [Training: 77%]\tLoss: 0.054307\n",
      "Epoch: 10 [Training: 80%]\tLoss: 0.009324\n",
      "Epoch: 10 [Training: 83%]\tLoss: 0.008530\n",
      "Epoch: 10 [Training: 86%]\tLoss: 0.061253\n",
      "Epoch: 10 [Training: 90%]\tLoss: 0.050486\n",
      "Epoch: 10 [Training: 93%]\tLoss: 0.019373\n",
      "Epoch: 10 [Training: 96%]\tLoss: 0.002519\n",
      "Epoch: 10 [Training: 99%]\tLoss: 0.058812\n",
      "\n",
      "Test set: Average loss: 0.0360, Accuracy: 9882/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # iterate through each worker's dataset\n",
    "        \n",
    "        model.send(data.location) #send the model to the right location\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # 1) erase previous gradients (if they exist)\n",
    "        output = model(data)  # 2) make a prediction\n",
    "        loss = F.nll_loss(output, target)  # 3) calculate how much we missed\n",
    "        loss.backward()  # 4) figure out which weights caused us to miss\n",
    "        optimizer.step()  # 5) change those weights\n",
    "        model.get()  # get the model back (with gradients)\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() #get the loss back\n",
    "            print('Epoch: {} [Training: {:.0f}%]\\tLoss: {:.6f}'.format(epoch, 100. * batch_idx / len(federated_train_loader), loss.item()))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Obtained 98.8200%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Obtained {:.4f}%\".format( 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
