{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-encrypted in ./anaconda3/lib/python3.6/site-packages (0.5.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.6/site-packages (from tf-encrypted) (5.1.1)\n",
      "Requirement already satisfied: tensorflow<2,>=1.12.0 in ./anaconda3/lib/python3.6/site-packages (from tf-encrypted) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in ./.local/lib/python3.6/site-packages (from tf-encrypted) (1.16.4)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.7.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.0.8)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.21.1)\n",
      "Requirement already satisfied: six>=1.10.0 in ./.local/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.11.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (3.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.1.7)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.14.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.31.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in ./anaconda3/lib/python3.6/site-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.1.0)\n",
      "Requirement already satisfied: h5py in ./anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted) (2.7.1)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./anaconda3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./anaconda3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted) (0.14.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Cloning into 'PySyft'...\n",
      "remote: Enumerating objects: 29220, done.\u001b[K\n",
      "remote: Total 29220 (delta 0), reused 0 (delta 0), pack-reused 29220\u001b[K\n",
      "Receiving objects: 100% (29220/29220), 32.18 MiB | 8.11 MiB/s, done.\n",
      "Resolving deltas: 100% (19444/19444), done.\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "Collecting lz4\n",
      "  Using cached https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Installing collected packages: lz4\n",
      "  Found existing installation: lz4 2.1.10\n",
      "    Uninstalling lz4-2.1.10:\n",
      "      Successfully uninstalled lz4-2.1.10\n",
      "Successfully installed lz4-2.1.10\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting websocket\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/6d/a60d620ea575c885510c574909d2e3ed62129b121fa2df00ca1c81024c87/websocket-0.2.1.tar.gz (195kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 555kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gevent (from websocket)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/ca/5b5962361ed832847b6b2f9a2d0452c8c2f29a93baef850bb8ad067c7bf9/gevent-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5MB 18.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting greenlet (from websocket)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: websocket\n",
      "  Building wheel for websocket (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/akshay/.cache/pip/wheels/35/f7/5c/9e8243838269ea93f05295708519a6e183fa6b515d9ce3b636\n",
      "Successfully built websocket\n",
      "Installing collected packages: greenlet, gevent, websocket\n",
      "  Found existing installation: greenlet 0.4.13\n",
      "\u001b[31mERROR: Cannot uninstall 'greenlet'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting websockets\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/d2/bf72435a7d56f94b57efdeae26c76bf0d16f409fd44ff595da745c3fbefd/websockets-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 666kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: websockets\n",
      "  Found existing installation: websockets 7.0\n",
      "    Uninstalling websockets-7.0:\n",
      "      Successfully uninstalled websockets-7.0\n",
      "Successfully installed websockets-8.0.1\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting zstd\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/37/6a7ba746ebddbd6cd06de84367515d6bc239acd94fb3e0b1c85788176ca2/zstd-1.4.1.0.tar.gz (454kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 569kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: zstd\n",
      "  Building wheel for zstd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/akshay/.cache/pip/wheels/66/3f/ee/ac08c81af7c1b24a80c746df669ea3cb37542d27877d66ccf4\n",
      "Successfully built zstd\n",
      "Installing collected packages: zstd\n",
      "  Found existing installation: zstd 1.4.0.0\n",
      "    Uninstalling zstd-1.4.0.0:\n",
      "      Successfully uninstalled zstd-1.4.0.0\n",
      "Successfully installed zstd-1.4.1.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-encrypted\n",
    "\n",
    "! URL=\"https://github.com/openmined/PySyft.git\" && FOLDER=\"PySyft\" && if [ ! -d $FOLDER ]; then git clone -b dev --single-branch $URL; else (cd $FOLDER && git pull $URL && cd ..); fi;\n",
    "\n",
    "!cd PySyft; python setup.py install  > /dev/null\n",
    "\n",
    "!pip install --upgrade --force-reinstall lz4\n",
    "!pip install --upgrade --force-reinstall websocket\n",
    "!pip install --upgrade --force-reinstall websockets\n",
    "!pip install --upgrade --force-reinstall zstd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Create a couple of workers\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  \n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7d78627c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/akshay/.pytorch/F_MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:03, 2578883.18it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akshay/.pytorch/F_MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/akshay/.pytorch/F_MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 45502.42it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akshay/.pytorch/F_MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/akshay/.pytorch/F_MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 838737.84it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akshay/.pytorch/F_MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/akshay/.pytorch/F_MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 17345.56it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/akshay/.pytorch/F_MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]) \n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/F_MNIST_data/', train=True, download=True, transform=transform).federate((bob, alice))\n",
    "mnist_testset = datasets.MNIST('~/.pytorch/F_MNIST_data/', train=False, download=True, transform=transform)\n",
    "\n",
    "federated_train_loader = sy.FederatedDataLoader(mnist_trainset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=args.test_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "   \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [Training: 0%]\tLoss: 2.307303\n",
      "Epoch: 1 [Training: 3%]\tLoss: 2.152380\n",
      "Epoch: 1 [Training: 6%]\tLoss: 1.851272\n",
      "Epoch: 1 [Training: 10%]\tLoss: 1.258177\n",
      "Epoch: 1 [Training: 13%]\tLoss: 0.785922\n",
      "Epoch: 1 [Training: 16%]\tLoss: 0.752820\n",
      "Epoch: 1 [Training: 19%]\tLoss: 0.389547\n",
      "Epoch: 1 [Training: 22%]\tLoss: 0.668079\n",
      "Epoch: 1 [Training: 26%]\tLoss: 0.556559\n",
      "Epoch: 1 [Training: 29%]\tLoss: 0.298134\n",
      "Epoch: 1 [Training: 32%]\tLoss: 0.709528\n",
      "Epoch: 1 [Training: 35%]\tLoss: 0.243506\n",
      "Epoch: 1 [Training: 38%]\tLoss: 0.295973\n",
      "Epoch: 1 [Training: 42%]\tLoss: 0.437727\n",
      "Epoch: 1 [Training: 45%]\tLoss: 0.444640\n",
      "Epoch: 1 [Training: 48%]\tLoss: 0.462919\n",
      "Epoch: 1 [Training: 51%]\tLoss: 0.377572\n",
      "Epoch: 1 [Training: 54%]\tLoss: 0.231817\n",
      "Epoch: 1 [Training: 58%]\tLoss: 0.164112\n",
      "Epoch: 1 [Training: 61%]\tLoss: 0.186264\n",
      "Epoch: 1 [Training: 64%]\tLoss: 0.267224\n",
      "Epoch: 1 [Training: 67%]\tLoss: 0.161648\n",
      "Epoch: 1 [Training: 70%]\tLoss: 0.223413\n",
      "Epoch: 1 [Training: 74%]\tLoss: 0.181516\n",
      "Epoch: 1 [Training: 77%]\tLoss: 0.177804\n",
      "Epoch: 1 [Training: 80%]\tLoss: 0.136097\n",
      "Epoch: 1 [Training: 83%]\tLoss: 0.146183\n",
      "Epoch: 1 [Training: 86%]\tLoss: 0.195801\n",
      "Epoch: 1 [Training: 90%]\tLoss: 0.309904\n",
      "Epoch: 1 [Training: 93%]\tLoss: 0.128512\n",
      "Epoch: 1 [Training: 96%]\tLoss: 0.146248\n",
      "Epoch: 1 [Training: 99%]\tLoss: 0.104152\n",
      "\n",
      "Test set: Average loss: 0.1696, Accuracy: 9487/10000 (95%)\n",
      "\n",
      "Epoch: 2 [Training: 0%]\tLoss: 0.108758\n",
      "Epoch: 2 [Training: 3%]\tLoss: 0.266778\n",
      "Epoch: 2 [Training: 6%]\tLoss: 0.050112\n",
      "Epoch: 2 [Training: 10%]\tLoss: 0.287405\n",
      "Epoch: 2 [Training: 13%]\tLoss: 0.163941\n",
      "Epoch: 2 [Training: 16%]\tLoss: 0.207662\n",
      "Epoch: 2 [Training: 19%]\tLoss: 0.148601\n",
      "Epoch: 2 [Training: 22%]\tLoss: 0.146983\n",
      "Epoch: 2 [Training: 26%]\tLoss: 0.127188\n",
      "Epoch: 2 [Training: 29%]\tLoss: 0.165691\n",
      "Epoch: 2 [Training: 32%]\tLoss: 0.272459\n",
      "Epoch: 2 [Training: 35%]\tLoss: 0.069282\n",
      "Epoch: 2 [Training: 38%]\tLoss: 0.108361\n",
      "Epoch: 2 [Training: 42%]\tLoss: 0.223120\n",
      "Epoch: 2 [Training: 45%]\tLoss: 0.169728\n",
      "Epoch: 2 [Training: 48%]\tLoss: 0.165921\n",
      "Epoch: 2 [Training: 51%]\tLoss: 0.174408\n",
      "Epoch: 2 [Training: 54%]\tLoss: 0.084015\n",
      "Epoch: 2 [Training: 58%]\tLoss: 0.093256\n",
      "Epoch: 2 [Training: 61%]\tLoss: 0.114428\n",
      "Epoch: 2 [Training: 64%]\tLoss: 0.050374\n",
      "Epoch: 2 [Training: 67%]\tLoss: 0.098446\n",
      "Epoch: 2 [Training: 70%]\tLoss: 0.213261\n",
      "Epoch: 2 [Training: 74%]\tLoss: 0.094775\n",
      "Epoch: 2 [Training: 77%]\tLoss: 0.135628\n",
      "Epoch: 2 [Training: 80%]\tLoss: 0.161076\n",
      "Epoch: 2 [Training: 83%]\tLoss: 0.035513\n",
      "Epoch: 2 [Training: 86%]\tLoss: 0.084948\n",
      "Epoch: 2 [Training: 90%]\tLoss: 0.041469\n",
      "Epoch: 2 [Training: 93%]\tLoss: 0.199830\n",
      "Epoch: 2 [Training: 96%]\tLoss: 0.041500\n",
      "Epoch: 2 [Training: 99%]\tLoss: 0.109184\n",
      "\n",
      "Test set: Average loss: 0.0885, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Epoch: 3 [Training: 0%]\tLoss: 0.048438\n",
      "Epoch: 3 [Training: 3%]\tLoss: 0.149034\n",
      "Epoch: 3 [Training: 6%]\tLoss: 0.049779\n",
      "Epoch: 3 [Training: 10%]\tLoss: 0.047878\n",
      "Epoch: 3 [Training: 13%]\tLoss: 0.099967\n",
      "Epoch: 3 [Training: 16%]\tLoss: 0.075994\n",
      "Epoch: 3 [Training: 19%]\tLoss: 0.092103\n",
      "Epoch: 3 [Training: 22%]\tLoss: 0.206135\n",
      "Epoch: 3 [Training: 26%]\tLoss: 0.103247\n",
      "Epoch: 3 [Training: 29%]\tLoss: 0.013604\n",
      "Epoch: 3 [Training: 32%]\tLoss: 0.125244\n",
      "Epoch: 3 [Training: 35%]\tLoss: 0.166772\n",
      "Epoch: 3 [Training: 38%]\tLoss: 0.104184\n",
      "Epoch: 3 [Training: 42%]\tLoss: 0.045272\n",
      "Epoch: 3 [Training: 45%]\tLoss: 0.081761\n",
      "Epoch: 3 [Training: 48%]\tLoss: 0.058571\n",
      "Epoch: 3 [Training: 51%]\tLoss: 0.161980\n",
      "Epoch: 3 [Training: 54%]\tLoss: 0.068453\n",
      "Epoch: 3 [Training: 58%]\tLoss: 0.038911\n",
      "Epoch: 3 [Training: 61%]\tLoss: 0.073067\n",
      "Epoch: 3 [Training: 64%]\tLoss: 0.059561\n",
      "Epoch: 3 [Training: 67%]\tLoss: 0.123575\n",
      "Epoch: 3 [Training: 70%]\tLoss: 0.033728\n",
      "Epoch: 3 [Training: 74%]\tLoss: 0.073370\n",
      "Epoch: 3 [Training: 77%]\tLoss: 0.087765\n",
      "Epoch: 3 [Training: 80%]\tLoss: 0.069232\n",
      "Epoch: 3 [Training: 83%]\tLoss: 0.195307\n",
      "Epoch: 3 [Training: 86%]\tLoss: 0.085394\n",
      "Epoch: 3 [Training: 90%]\tLoss: 0.041543\n",
      "Epoch: 3 [Training: 93%]\tLoss: 0.030729\n",
      "Epoch: 3 [Training: 96%]\tLoss: 0.022778\n",
      "Epoch: 3 [Training: 99%]\tLoss: 0.008504\n",
      "\n",
      "Test set: Average loss: 0.0657, Accuracy: 9803/10000 (98%)\n",
      "\n",
      "Epoch: 4 [Training: 0%]\tLoss: 0.143218\n",
      "Epoch: 4 [Training: 3%]\tLoss: 0.013475\n",
      "Epoch: 4 [Training: 6%]\tLoss: 0.060046\n",
      "Epoch: 4 [Training: 10%]\tLoss: 0.041011\n",
      "Epoch: 4 [Training: 13%]\tLoss: 0.110411\n",
      "Epoch: 4 [Training: 16%]\tLoss: 0.064776\n",
      "Epoch: 4 [Training: 19%]\tLoss: 0.134636\n",
      "Epoch: 4 [Training: 22%]\tLoss: 0.071463\n",
      "Epoch: 4 [Training: 26%]\tLoss: 0.053570\n",
      "Epoch: 4 [Training: 29%]\tLoss: 0.030522\n",
      "Epoch: 4 [Training: 32%]\tLoss: 0.049039\n",
      "Epoch: 4 [Training: 35%]\tLoss: 0.059109\n",
      "Epoch: 4 [Training: 38%]\tLoss: 0.022831\n",
      "Epoch: 4 [Training: 42%]\tLoss: 0.028019\n",
      "Epoch: 4 [Training: 45%]\tLoss: 0.064245\n",
      "Epoch: 4 [Training: 48%]\tLoss: 0.040430\n",
      "Epoch: 4 [Training: 51%]\tLoss: 0.067668\n",
      "Epoch: 4 [Training: 54%]\tLoss: 0.101321\n",
      "Epoch: 4 [Training: 58%]\tLoss: 0.030789\n",
      "Epoch: 4 [Training: 61%]\tLoss: 0.015031\n",
      "Epoch: 4 [Training: 64%]\tLoss: 0.017741\n",
      "Epoch: 4 [Training: 67%]\tLoss: 0.021863\n",
      "Epoch: 4 [Training: 70%]\tLoss: 0.049477\n",
      "Epoch: 4 [Training: 74%]\tLoss: 0.018333\n",
      "Epoch: 4 [Training: 77%]\tLoss: 0.042879\n",
      "Epoch: 4 [Training: 80%]\tLoss: 0.041446\n",
      "Epoch: 4 [Training: 83%]\tLoss: 0.036025\n",
      "Epoch: 4 [Training: 86%]\tLoss: 0.086534\n",
      "Epoch: 4 [Training: 90%]\tLoss: 0.070694\n",
      "Epoch: 4 [Training: 93%]\tLoss: 0.025137\n",
      "Epoch: 4 [Training: 96%]\tLoss: 0.137756\n",
      "Epoch: 4 [Training: 99%]\tLoss: 0.068094\n",
      "\n",
      "Test set: Average loss: 0.0539, Accuracy: 9831/10000 (98%)\n",
      "\n",
      "Epoch: 5 [Training: 0%]\tLoss: 0.082655\n",
      "Epoch: 5 [Training: 3%]\tLoss: 0.039600\n",
      "Epoch: 5 [Training: 6%]\tLoss: 0.068578\n",
      "Epoch: 5 [Training: 10%]\tLoss: 0.162828\n",
      "Epoch: 5 [Training: 13%]\tLoss: 0.028125\n",
      "Epoch: 5 [Training: 16%]\tLoss: 0.018763\n",
      "Epoch: 5 [Training: 19%]\tLoss: 0.044665\n",
      "Epoch: 5 [Training: 22%]\tLoss: 0.014191\n",
      "Epoch: 5 [Training: 26%]\tLoss: 0.042796\n",
      "Epoch: 5 [Training: 29%]\tLoss: 0.110474\n",
      "Epoch: 5 [Training: 32%]\tLoss: 0.047643\n",
      "Epoch: 5 [Training: 35%]\tLoss: 0.016220\n",
      "Epoch: 5 [Training: 38%]\tLoss: 0.111569\n",
      "Epoch: 5 [Training: 42%]\tLoss: 0.129427\n",
      "Epoch: 5 [Training: 45%]\tLoss: 0.081558\n",
      "Epoch: 5 [Training: 48%]\tLoss: 0.049033\n",
      "Epoch: 5 [Training: 51%]\tLoss: 0.032722\n",
      "Epoch: 5 [Training: 54%]\tLoss: 0.045952\n",
      "Epoch: 5 [Training: 58%]\tLoss: 0.073073\n",
      "Epoch: 5 [Training: 61%]\tLoss: 0.109228\n",
      "Epoch: 5 [Training: 64%]\tLoss: 0.051607\n",
      "Epoch: 5 [Training: 67%]\tLoss: 0.021612\n",
      "Epoch: 5 [Training: 70%]\tLoss: 0.007918\n",
      "Epoch: 5 [Training: 74%]\tLoss: 0.047501\n",
      "Epoch: 5 [Training: 77%]\tLoss: 0.031786\n",
      "Epoch: 5 [Training: 80%]\tLoss: 0.031934\n",
      "Epoch: 5 [Training: 83%]\tLoss: 0.036741\n",
      "Epoch: 5 [Training: 86%]\tLoss: 0.014268\n",
      "Epoch: 5 [Training: 90%]\tLoss: 0.081360\n",
      "Epoch: 5 [Training: 93%]\tLoss: 0.060367\n",
      "Epoch: 5 [Training: 96%]\tLoss: 0.162455\n",
      "Epoch: 5 [Training: 99%]\tLoss: 0.065688\n",
      "\n",
      "Test set: Average loss: 0.0511, Accuracy: 9831/10000 (98%)\n",
      "\n",
      "Epoch: 6 [Training: 0%]\tLoss: 0.019212\n",
      "Epoch: 6 [Training: 3%]\tLoss: 0.003931\n",
      "Epoch: 6 [Training: 6%]\tLoss: 0.023741\n",
      "Epoch: 6 [Training: 10%]\tLoss: 0.104028\n",
      "Epoch: 6 [Training: 13%]\tLoss: 0.026281\n",
      "Epoch: 6 [Training: 16%]\tLoss: 0.144363\n",
      "Epoch: 6 [Training: 19%]\tLoss: 0.039494\n",
      "Epoch: 6 [Training: 22%]\tLoss: 0.038643\n",
      "Epoch: 6 [Training: 26%]\tLoss: 0.023662\n",
      "Epoch: 6 [Training: 29%]\tLoss: 0.023237\n",
      "Epoch: 6 [Training: 32%]\tLoss: 0.115795\n",
      "Epoch: 6 [Training: 35%]\tLoss: 0.019793\n",
      "Epoch: 6 [Training: 38%]\tLoss: 0.066402\n",
      "Epoch: 6 [Training: 42%]\tLoss: 0.065943\n",
      "Epoch: 6 [Training: 45%]\tLoss: 0.021108\n",
      "Epoch: 6 [Training: 48%]\tLoss: 0.011036\n",
      "Epoch: 6 [Training: 51%]\tLoss: 0.057914\n",
      "Epoch: 6 [Training: 54%]\tLoss: 0.036496\n",
      "Epoch: 6 [Training: 58%]\tLoss: 0.034058\n",
      "Epoch: 6 [Training: 61%]\tLoss: 0.006872\n",
      "Epoch: 6 [Training: 64%]\tLoss: 0.033127\n",
      "Epoch: 6 [Training: 67%]\tLoss: 0.072500\n",
      "Epoch: 6 [Training: 70%]\tLoss: 0.035727\n",
      "Epoch: 6 [Training: 74%]\tLoss: 0.019486\n",
      "Epoch: 6 [Training: 77%]\tLoss: 0.019336\n",
      "Epoch: 6 [Training: 80%]\tLoss: 0.090589\n",
      "Epoch: 6 [Training: 83%]\tLoss: 0.060641\n",
      "Epoch: 6 [Training: 86%]\tLoss: 0.050454\n",
      "Epoch: 6 [Training: 90%]\tLoss: 0.005014\n",
      "Epoch: 6 [Training: 93%]\tLoss: 0.034053\n",
      "Epoch: 6 [Training: 96%]\tLoss: 0.047466\n",
      "Epoch: 6 [Training: 99%]\tLoss: 0.019918\n",
      "\n",
      "Test set: Average loss: 0.0465, Accuracy: 9853/10000 (99%)\n",
      "\n",
      "Epoch: 7 [Training: 0%]\tLoss: 0.056733\n",
      "Epoch: 7 [Training: 3%]\tLoss: 0.031696\n",
      "Epoch: 7 [Training: 6%]\tLoss: 0.071479\n",
      "Epoch: 7 [Training: 10%]\tLoss: 0.017623\n",
      "Epoch: 7 [Training: 13%]\tLoss: 0.009857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 [Training: 16%]\tLoss: 0.039876\n",
      "Epoch: 7 [Training: 19%]\tLoss: 0.014503\n",
      "Epoch: 7 [Training: 22%]\tLoss: 0.074501\n",
      "Epoch: 7 [Training: 26%]\tLoss: 0.004399\n",
      "Epoch: 7 [Training: 29%]\tLoss: 0.027279\n",
      "Epoch: 7 [Training: 32%]\tLoss: 0.026506\n",
      "Epoch: 7 [Training: 35%]\tLoss: 0.014903\n",
      "Epoch: 7 [Training: 38%]\tLoss: 0.086426\n",
      "Epoch: 7 [Training: 42%]\tLoss: 0.024283\n",
      "Epoch: 7 [Training: 45%]\tLoss: 0.015972\n",
      "Epoch: 7 [Training: 48%]\tLoss: 0.087584\n",
      "Epoch: 7 [Training: 51%]\tLoss: 0.054718\n",
      "Epoch: 7 [Training: 54%]\tLoss: 0.057633\n",
      "Epoch: 7 [Training: 58%]\tLoss: 0.004831\n",
      "Epoch: 7 [Training: 61%]\tLoss: 0.016346\n",
      "Epoch: 7 [Training: 64%]\tLoss: 0.004659\n",
      "Epoch: 7 [Training: 67%]\tLoss: 0.081221\n",
      "Epoch: 7 [Training: 70%]\tLoss: 0.010923\n",
      "Epoch: 7 [Training: 74%]\tLoss: 0.062022\n",
      "Epoch: 7 [Training: 77%]\tLoss: 0.108107\n",
      "Epoch: 7 [Training: 80%]\tLoss: 0.097467\n",
      "Epoch: 7 [Training: 83%]\tLoss: 0.021479\n",
      "Epoch: 7 [Training: 86%]\tLoss: 0.022884\n",
      "Epoch: 7 [Training: 90%]\tLoss: 0.016166\n",
      "Epoch: 7 [Training: 93%]\tLoss: 0.066763\n",
      "Epoch: 7 [Training: 96%]\tLoss: 0.112641\n",
      "Epoch: 7 [Training: 99%]\tLoss: 0.054449\n",
      "\n",
      "Test set: Average loss: 0.0399, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "Epoch: 8 [Training: 0%]\tLoss: 0.018849\n",
      "Epoch: 8 [Training: 3%]\tLoss: 0.102302\n",
      "Epoch: 8 [Training: 6%]\tLoss: 0.031594\n",
      "Epoch: 8 [Training: 10%]\tLoss: 0.018197\n",
      "Epoch: 8 [Training: 13%]\tLoss: 0.083210\n",
      "Epoch: 8 [Training: 16%]\tLoss: 0.040422\n",
      "Epoch: 8 [Training: 19%]\tLoss: 0.029943\n",
      "Epoch: 8 [Training: 22%]\tLoss: 0.036008\n",
      "Epoch: 8 [Training: 26%]\tLoss: 0.028162\n",
      "Epoch: 8 [Training: 29%]\tLoss: 0.019329\n",
      "Epoch: 8 [Training: 32%]\tLoss: 0.034303\n",
      "Epoch: 8 [Training: 35%]\tLoss: 0.009494\n",
      "Epoch: 8 [Training: 38%]\tLoss: 0.044156\n",
      "Epoch: 8 [Training: 42%]\tLoss: 0.053731\n",
      "Epoch: 8 [Training: 45%]\tLoss: 0.007199\n",
      "Epoch: 8 [Training: 48%]\tLoss: 0.022932\n",
      "Epoch: 8 [Training: 51%]\tLoss: 0.013671\n",
      "Epoch: 8 [Training: 54%]\tLoss: 0.048986\n",
      "Epoch: 8 [Training: 58%]\tLoss: 0.052141\n",
      "Epoch: 8 [Training: 61%]\tLoss: 0.014470\n",
      "Epoch: 8 [Training: 64%]\tLoss: 0.085063\n",
      "Epoch: 8 [Training: 67%]\tLoss: 0.066029\n",
      "Epoch: 8 [Training: 70%]\tLoss: 0.005550\n",
      "Epoch: 8 [Training: 74%]\tLoss: 0.178319\n",
      "Epoch: 8 [Training: 77%]\tLoss: 0.024497\n",
      "Epoch: 8 [Training: 80%]\tLoss: 0.006748\n",
      "Epoch: 8 [Training: 83%]\tLoss: 0.008978\n",
      "Epoch: 8 [Training: 86%]\tLoss: 0.046445\n",
      "Epoch: 8 [Training: 90%]\tLoss: 0.010649\n",
      "Epoch: 8 [Training: 93%]\tLoss: 0.009854\n",
      "Epoch: 8 [Training: 96%]\tLoss: 0.142035\n",
      "Epoch: 8 [Training: 99%]\tLoss: 0.025941\n",
      "\n",
      "Test set: Average loss: 0.0494, Accuracy: 9834/10000 (98%)\n",
      "\n",
      "Epoch: 9 [Training: 0%]\tLoss: 0.011256\n",
      "Epoch: 9 [Training: 3%]\tLoss: 0.042155\n",
      "Epoch: 9 [Training: 6%]\tLoss: 0.014511\n",
      "Epoch: 9 [Training: 10%]\tLoss: 0.005187\n",
      "Epoch: 9 [Training: 13%]\tLoss: 0.030799\n",
      "Epoch: 9 [Training: 16%]\tLoss: 0.018657\n",
      "Epoch: 9 [Training: 19%]\tLoss: 0.005129\n",
      "Epoch: 9 [Training: 22%]\tLoss: 0.016094\n",
      "Epoch: 9 [Training: 26%]\tLoss: 0.022836\n",
      "Epoch: 9 [Training: 29%]\tLoss: 0.026814\n",
      "Epoch: 9 [Training: 32%]\tLoss: 0.031623\n",
      "Epoch: 9 [Training: 35%]\tLoss: 0.134087\n",
      "Epoch: 9 [Training: 38%]\tLoss: 0.012761\n",
      "Epoch: 9 [Training: 42%]\tLoss: 0.003959\n",
      "Epoch: 9 [Training: 45%]\tLoss: 0.005524\n",
      "Epoch: 9 [Training: 48%]\tLoss: 0.064085\n",
      "Epoch: 9 [Training: 51%]\tLoss: 0.031737\n",
      "Epoch: 9 [Training: 54%]\tLoss: 0.011293\n",
      "Epoch: 9 [Training: 58%]\tLoss: 0.012573\n",
      "Epoch: 9 [Training: 61%]\tLoss: 0.010968\n",
      "Epoch: 9 [Training: 64%]\tLoss: 0.055998\n",
      "Epoch: 9 [Training: 67%]\tLoss: 0.062934\n",
      "Epoch: 9 [Training: 70%]\tLoss: 0.056225\n",
      "Epoch: 9 [Training: 74%]\tLoss: 0.083464\n",
      "Epoch: 9 [Training: 77%]\tLoss: 0.011868\n",
      "Epoch: 9 [Training: 80%]\tLoss: 0.013776\n",
      "Epoch: 9 [Training: 83%]\tLoss: 0.072485\n",
      "Epoch: 9 [Training: 86%]\tLoss: 0.039236\n",
      "Epoch: 9 [Training: 90%]\tLoss: 0.025850\n",
      "Epoch: 9 [Training: 93%]\tLoss: 0.075540\n",
      "Epoch: 9 [Training: 96%]\tLoss: 0.011663\n",
      "Epoch: 9 [Training: 99%]\tLoss: 0.043383\n",
      "\n",
      "Test set: Average loss: 0.0420, Accuracy: 9870/10000 (99%)\n",
      "\n",
      "Epoch: 10 [Training: 0%]\tLoss: 0.097661\n",
      "Epoch: 10 [Training: 3%]\tLoss: 0.008306\n",
      "Epoch: 10 [Training: 6%]\tLoss: 0.028625\n",
      "Epoch: 10 [Training: 10%]\tLoss: 0.030273\n",
      "Epoch: 10 [Training: 13%]\tLoss: 0.012451\n",
      "Epoch: 10 [Training: 16%]\tLoss: 0.017396\n",
      "Epoch: 10 [Training: 19%]\tLoss: 0.027343\n",
      "Epoch: 10 [Training: 22%]\tLoss: 0.108637\n",
      "Epoch: 10 [Training: 26%]\tLoss: 0.022504\n",
      "Epoch: 10 [Training: 29%]\tLoss: 0.009166\n",
      "Epoch: 10 [Training: 32%]\tLoss: 0.007093\n",
      "Epoch: 10 [Training: 35%]\tLoss: 0.003321\n",
      "Epoch: 10 [Training: 38%]\tLoss: 0.008056\n",
      "Epoch: 10 [Training: 42%]\tLoss: 0.028249\n",
      "Epoch: 10 [Training: 45%]\tLoss: 0.009685\n",
      "Epoch: 10 [Training: 48%]\tLoss: 0.024188\n",
      "Epoch: 10 [Training: 51%]\tLoss: 0.051361\n",
      "Epoch: 10 [Training: 54%]\tLoss: 0.040366\n",
      "Epoch: 10 [Training: 58%]\tLoss: 0.012233\n",
      "Epoch: 10 [Training: 61%]\tLoss: 0.022487\n",
      "Epoch: 10 [Training: 64%]\tLoss: 0.005419\n",
      "Epoch: 10 [Training: 67%]\tLoss: 0.016321\n",
      "Epoch: 10 [Training: 70%]\tLoss: 0.003712\n",
      "Epoch: 10 [Training: 74%]\tLoss: 0.013479\n",
      "Epoch: 10 [Training: 77%]\tLoss: 0.055972\n",
      "Epoch: 10 [Training: 80%]\tLoss: 0.009183\n",
      "Epoch: 10 [Training: 83%]\tLoss: 0.008562\n",
      "Epoch: 10 [Training: 86%]\tLoss: 0.061928\n",
      "Epoch: 10 [Training: 90%]\tLoss: 0.049334\n",
      "Epoch: 10 [Training: 93%]\tLoss: 0.020001\n",
      "Epoch: 10 [Training: 96%]\tLoss: 0.002624\n",
      "Epoch: 10 [Training: 99%]\tLoss: 0.060488\n",
      "\n",
      "Test set: Average loss: 0.0359, Accuracy: 9880/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # iterate through each worker's dataset\n",
    "        \n",
    "        model.send(data.location) #send the model to the right location\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # 1) erase previous gradients (if they exist)\n",
    "        output = model(data)  # 2) make a prediction\n",
    "        loss = F.nll_loss(output, target)  # 3) calculate how much we missed\n",
    "        loss.backward()  # 4) figure out which weights caused us to miss\n",
    "        optimizer.step()  # 5) change those weights\n",
    "        model.get()  # get the model back (with gradients)\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() #get the loss back\n",
    "            print('Epoch: {} [Training: {:.0f}%]\\tLoss: {:.6f}'.format(epoch, 100. * batch_idx / len(federated_train_loader), loss.item()))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Obtained 98.8000%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Obtained {:.4f}%\".format( 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
